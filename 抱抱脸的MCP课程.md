# HF MCP Course

[hf-mcp-course](https://huggingface.co/learn/mcp-course/unit0/introduction)

## Part1.  Introduction to Model Context Protocol (MCP)

### what Model Context Protocol is and why it’s important MCP概念

模型往往受限于训练数据，无法获取实时信息或调用专业工具，导致在许多场景下难以为用户提供真正相关、准确且有用的回答。

Model Context Protocol（MCP）使 AI 模型能够连接外部数据源、工具与环境，实现 AI 系统与更广阔数字世界之间的信息及能力无缝流转。这种互操作性对于培育并推广真正实用的 AI 应用至关重要。

### The integration challenges that MCP solves

集成难题:是在没有统一标准的情况下，将 M 种不同的 AI 应用逐一接入 N 种外部工具或数据源所面临的挑战.

没有MCP时：

    每个AI应用都要单独对接每个工具，就像每个手机都要配专属的充电器。
    如果有10个AI应用和10个工具，就需要做10×10=100种不同的对接方式，非常麻烦。

有了MCP后：

    所有AI应用统一用同一种"插头"（MCP协议）去连接工具。
    现在只需要：10个AI应用各实现一次MCP客户端 + 10个工具各实现一次MCP服务端 = 总共20次实现。

### The key concepts and terminology associated with MCP MCP的核心概念和组件

核心概念 : MCP 与 HTTP 或 USB-C 一样，是一套标准协议，用于将 AI 应用连接到外部工具与数据源。因此，使用统一的术语对于 MCP 的高效运作至关重要。

    标准化接口：解决重复对接问题
    明确分工：Host-Client-Server三层结构
    四大能力：Tools/Resources/Prompts/Sampling覆盖AI应用常见需求


### A simple example of MCP integration in action

![case-for-a-code-agent](images/case-for-a-code-agent.png)

上图展示了 Tools、Resources、Prompts、Sampling 这四种实体/能力(Entity)是如何被组合起来，共同支撑一个『代码智能体（code agent）』这一具体场景的.

| 实体/能力 类型                 | 名称                      | 说明                                          |
| -------------------- | ----------------------- | ------------------------------------------- |
| Tool（工具）         | Code Interpreter（代码解释器） | 可以执行 LLM 所编写代码的工具。                          |
| Resource（资源）     | Documentation（文档）       | 包含该应用自身文档的只读数据源，供 LLM 查阅。                   |
| Prompt     | Code Style（代码风格）        | 一段预置提示，引导 LLM 按既定规范生成代码。                    |
| Sampling（采样/再调用） | Code Review（代码审查）       | 允许服务器触发主机再次调用 LLM，让模型自己审阅已生成代码并决定是否需要进一步修改。 |

**在 MCP 的语境里，entities 并不是指“实体”这种抽象概念，而是 “暴露给 AI 的四类具体能力条目” 的统称。**

### MCP 的架构组成

![MCP-Architecture](images/MCP-Architecture.png)

架构 :

| MCP术语      | 描述       | 作用                       |
| ---------- | -------- | ------------------------ |
| Host   | 用户直接使用的AI应用（比如Claude桌面版）      | 主机负责管理用户交互与权限,通过 MCP 客户端发起与 MCP 服务器的连接,协调用户请求、大模型处理与外部工具之间的整体流程,以一致、连贯的格式向用户呈现结果。 |
| Client | Host内部负责和某个具体工具对接的小模块 | 个客户端与单个服务器保持 1:1 连接，负责处理 MCP 协议层面的通信细节，充当主机逻辑与外部服务器之间的中介。    |
| Server | 通过 MCP 协议暴露能力（工具、资源、提示词）的外部程序或服务.    |     提供对特定外部工具、数据源或服务的访问,作为现有功能的轻量级封装,可在本地（与主机同一台机器）或远程（通过网络）运行,以标准化格式暴露自身能力，供客户端发现与使用      |

注意：很多人把Host和Client混为一谈，但严格来说：

    Host是完整的AI应用（比如整个Claude软件）
    Client只是Host内部的一个小模块，专门负责对接某个工具

### MCP 组件交互

1. 用户交互:用户与主机（Host）应用交互，表达自己的意图或提出查询。
2. 主机处理:主机解析用户输入，可能调用大语言模型（LLM）来理解请求，并判断是否需要外部能力。
3. 客户端连接:主机指示其客户端（Client）组件连接到合适的服务器（Server）。
4. 能力发现:客户端向服务器查询，了解其提供了哪些能力（工具、资源、提示词）。
5. 能力调用:根据用户需求或 LLM 的判断，主机让客户端调用服务器上的特定能力。
6. 服务器执行:服务器执行所请求的功能，并将结果返回给客户端。
7. 结果整合:客户端将结果回传给主机，主机将结果纳入 LLM 的上下文或直接呈现给用户。

这些交互模式背后遵循着若干关键原则，持续塑造并推动 MCP 的设计与演进：

    标准化：为 AI 连接提供通用协议
    简洁性：核心协议保持简单，同时支持高级特性
    安全性：敏感操作须经用户明确授权
    可发现性：能力可被动态发现
    可扩展性：通过版本控制与能力协商支持协议演进
    互操作性：确保不同实现、不同环境之间均可协同工作

### MCP 的通信协议

MCP 的核心采用 JSON-RPC 2.0 作为客户端与服务器之间所有通信的消息格式。JSON-RPC 是一种轻量级、以 JSON 编码的远程过程调用协议，具备以下优势：

    人类可读，调试方便
    语言无关，可在任何编程环境中实现
    规范成熟，文档清晰，已被广泛采纳

MCP的通信协议定义了三种消息类型:请求（Requests）, 响应（Responses）和 通知（Notifications）.

#### 请求

 一般由客户端发往服务器，用于启动一次操作。一条请求消息包含：

    唯一的标识符（id）
    要调用的方法名（例如 tools/call）
    该方法的参数（如有）

示例:

```json
{
  "jsonrpc": "2.0",
  "id": 1,
  "method": "tools/call",
  "params": {
    "name": "weather",
    "arguments": {
      "location": "San Francisco"
    }
  }
}
```

#### 响应（Responses）

由服务器发往客户端，作为对请求的答复。一条响应消息包含：

    • 与对应请求相同的 id
    • 成功时返回结果（result），失败时返回错误（error）

成功响应示例：

```JSON
{
  "jsonrpc": "2.0",
  "id": 1,
  "result": {
    "temperature": 62,
    "conditions": "Partly cloudy"
  }
}
```

错误响应示例：

```json
{
  "jsonrpc": "2.0",
  "id": 1,
  "error": {
    "code": -32602,
    "message": "Invalid location parameter"
  }
}
```

#### 消息

单向消息，无需回复。通常由服务器发往客户端，用于提供更新或事件通知。
通知示例：

```json
{
  "jsonrpc": "2.0",
  "method": "progress",
  "params": {
    "message": "Processing data...",
    "percent": 50
  }
}
```

### MCP 的传输机制

JSON-RPC 只定义了消息格式，MCP 进一步规定了这些消息在客户端与服务器之间如何传输。目前支持两种主要传输方式：

stdio（标准输入/输出）

    适用于本地通信场景，即客户端与服务器运行在同一台机器上：
    • 主机（Host）应用将服务器作为子进程启动，通过向该进程的标准输入（stdin）写入数据、并从其标准输出（stdout）读取数据来进行通信。
    • 典型用例：访问本地文件系统、运行本地脚本等工具。
    • 主要优点：实现简单、无需网络配置、由操作系统天然提供安全沙箱隔离。
    
HTTP + SSE（Server-Sent Events）/ 可流式 HTTP

    适用于远程通信场景，即客户端与服务器可能位于不同机器：
    • 通信基于 HTTP，服务器利用 Server-Sent Events（SSE）在一条持久连接上向客户端推送更新。
    • 典型用例：连接远程 API、云服务平台或共享资源。
    • 主要优点：跨网络工作、易于与现有 Web 服务集成、兼容无服务器（serverless）环境。
    MCP 标准的最新更新还引入了“可流式 HTTP”（Streamable HTTP）：服务器可在需要时动态升级为 SSE 流式传输，同时保持对无服务器环境的兼容性，从而获得更高的灵活性。

### MCP 交互生命周期

#### 初始化

客户端连接到服务器，双方交换协议版本与能力信息；服务器回应其支持的协议版本与能力。客户端通过一条通知消息确认初始化完成。

    💻 → initialize 🌐
    💻 ← response 🌐
    💻 → initialized 🌐


#### 发现

客户端请求可用的能力信息，服务器返回可用工具的列表。

    💻 → tools/list 🌐
    💻 ← response 🌐

#### 执行

客户端根据主机需求调用能力。

    💻 → tools/call 🌐
    💻 ← notification（可选进度） 🌐
    💻 ← response 🌐

#### 终止

当不再需要连接时，连接被优雅关闭，服务器确认关闭请求。

    💻 → shutdown 🌐
    💻 ← response 🌐
    💻 → exit 🌐

### 理解 MCP 的能力

|Capability | 说明                                                                  | 示例                                  |
| -------------- | ------------------------------------------------------------------- | ----------------------------------- |
| Tools（工具）      | 可由 AI 模型主动调用的可执行函数，用于执行特定动作或获取计算后的数据，通常与具体业务场景紧密相关。                 | 天气应用提供的“根据城市名返回实时天气”函数。             |
| Resources（资源）  | 只读数据源，为对话或任务提供背景信息，无需复杂计算。                                          | 科研助手暴露的“最新 arXiv 论文列表”只读接口。         |
| Prompts（提示模板）  | 预定义的交互模板或工作流，用于引导用户、AI 模型与可用能力之间的协作。                                | “把下面这段文字压缩成 100 字以内”的总结提示。          |
| Sampling（采样）   | 由服务器发起、让客户端/主机再次调用 LLM 的机制，实现递归式 AI 协作：LLM 先生成内容，再自我审视并决定是否需要进一步迭代。 | 写作应用在生成文章后，自行决定“再润色一次”并调用 LLM 继续优化。 |

#### 工具（Tools）

工具是 AI 模型可通过 MCP 协议调用的可执行函数或操作。

    控制权：工具通常由 模型控制，即 AI 模型（LLM）根据用户请求与上下文决定是否调用。
    安全性：由于工具可能产生副作用，执行通常需要用户明确批准。
    典型用例：发送消息、创建工单、调用 API、执行计算。

示例：查询某地点当前天气的工具

#### 资源（Resources）

资源提供对数据源的只读访问，使 AI 模型能够检索上下文而无需执行复杂逻辑。

    控制权：资源由 应用控制，通常由主机应用决定何时访问。
    性质：仅用于数据检索，计算量极小，类似 REST API 的 GET 端点。
    安全性：只读特性使其风险低于工具。
    典型用例：读取文件内容、检索数据库记录、获取配置信息。

示例：读取文件内容的资源

#### 提示词（Prompts）

提示词是预定义的模板或工作流，用于引导用户、AI 模型与服务器能力之间的交互。

    控制权：提示词由 用户控制，通常以主机应用界面中的选项形式呈现。
    目的：为充分利用可用工具与资源而结构化交互。
    选择：用户通常在 AI 模型开始处理前选择提示词，为交互设定上下文。
    典型用例：常见工作流、专项任务模板、引导式交互。

示例：用于生成代码审查的提示模板

#### 采样（Sampling）

采样允许服务器请求客户端（具体为主机应用）执行 LLM 交互。

    控制权：由 服务器发起，但需要客户端/主机配合。
    目的：实现服务器驱动的智能体行为，支持递归或多步交互。
    安全性：与工具类似，采样操作通常需要用户批准。
    典型用例：复杂多步任务、自主智能体工作流、交互式流程。

示例：服务器请求客户端分析其处理过的数据

采样流程如下：

    服务器向客户端发送 sampling/createMessage 请求
    客户端审核并可修改该请求
    客户端从 LLM 采样
    客户端审核生成结果
    客户端将结果返回给服务器

这种人机协同设计确保用户对 LLM 所见与所生成内容保持控制。实现采样时，应提供清晰、结构良好的提示词，并包含相关上下文。

#### 能力协同

| 能力  | 控制方     | 控制方向            | 副作用   | 需审批      | 典型用例           |
| --- | ------- | --------------- | ----- | -------- | -------------- |
| 工具  | 模型(LLM) | 客户端 → 服务器       | 有（潜在） | 是        | 动作、API 调用、数据操作 |
| 资源  | 应用      | 客户端 → 服务器       | 无（只读） | 通常否      | 数据检索、上下文收集     |
| 提示词 | 用户      | 服务器 → 客户端       | 无     | 否（由用户选择） | 引导式工作流、专项模板    |
| 采样  | 服务器     | 服务器 → 客户端 → 服务器 | 间接    | 是        | 多步任务、智能体行为     |

这些能力以互补方式协同：

    用户可先选择 提示词 启动专项工作流
    提示词可包含来自 资源 的上下文
    处理过程中，AI 模型可调用 工具 执行特定动作
    对于复杂操作，服务器可使用 采样 请求额外 LLM 处理

这些原语的区分提供了清晰的 MCP 交互结构，使 AI 模型在访问信息、执行动作、参与复杂工作流的同时，维持适当的控制边界。

MCP 的关键特性之一是动态能力发现。客户端连接到服务器后，可通过特定 list 方法查询可用工具、资源与提示词：

    tools/list：发现可用工具
    resources/list：发现可用资源
    prompts/list：发现可用提示词

这种动态发现机制使客户端无需硬编码服务器功能即可适应其具体能力。

## Part2. Build a MCP Application

注: 前面的基本都是课程的翻译,这一部分我看原课程有点懵,所以会根据我的理解重新组织, 但是请放心, 跟随下面的步骤,你一定可以在本地跑一个MCP.

前面提到,MCP的架构包括三个部分,面向用户的 Host, 提供具体服务的 Server, 以及链接 Host 和 Server 的 Client. 

### Server

首先定义一个小的 tool sentiment_analysis, 实现情感分析的功能. 接着使用 Gradio 将 tool 封装, 提供该 Gradio 的接口, 实现 Server, 使得用户和大模型都可以通过该接口调用 Server sentiment_analysis. 

```py3

```


Client: 基于 MCP SDK 搭建该系统的 Client. Host 可以通过 tools 列出支持的工具, 通过 call_tool 调用前面的 Server sentiment_analysis

### Host

 课程里使用 Countinue (vscode 插件) 作为host, 所以 Host 主要的步骤是 Continue 配置大模型和 MCP 服务. 

1. 使用 ollma 运行大模型
```bash
nohup ollama serve &

ollama run hf.co/unsloth/Devstral-Small-2505-GGUF:Q4_K_M

```


| Host   | 用户直接使用的AI应用（比如Claude桌面版）      | 主机负责管理用户交互与权限,通过 MCP 客户端发起与 MCP 服务器的连接,协调用户请求、大模型处理与外部工具之间的整体流程,以一致、连贯的格式向用户呈现结果。 |
| Client | Host内部负责和某个具体工具对接的小模块 | 个客户端与单个服务器保持 1:1 连接，负责处理 MCP 协议层面的通信细节，充当主机逻辑与外部服务器之间的中介。    |
| Server | 通过 MCP 协议暴露能力（工具、资源、提示词）的外部程序或服务.    |     提供对特定外部工具、数据源或服务的访问,作为现有功能的轻量级封装,可在本地（与主机同一台机器）或远程（通过网络）运行,以标准化格式暴露自身能力，供客户端发现与使用      |


### Tool list

跑了一下课程里的例子，日新月异，一步步跑很多跑不起来，更多的还是看他提到了什么，再去看文档，再去修改。所以这里就给几个它用到的工具（很多基于Claude不太平民就不提了）。

| 技术/组件                                                     | 在 MCP 体系中的角色             | 实现功能                                                      | 关键依赖/接口                                                     |
| --------------------------------------------------------- | ------------------------ | --------------------------------------------------------- | ----------------------------------------------------------- |
| **Lemonade Server**                                       | **MCP Server (推理端)**     | 提供本地 LLM 推理服务，支持 CPU / Vulkan-AMD GPU / AMD NPU 加速        | llamacpp + Vulkan + AMD Ryzen™ AI 驱动                        |
| **Tiny Agents CLI**                                       | **MCP Client (Agent 端)** | 从 Lemonade Server 拉取模型 → 根据用户指令触发 MCP Tool 调用 → 汇总结果返回给用户 | 读取 `agent.json` 配置，通过 MCP-over-stdio 与 Desktop Commander 通信 |
| **Desktop Commander MCP Server**                          | **MCP Server (工具端)**     | 暴露本地文件系统、终端、代码编辑等 18 种工具接口给 Agent 调用                      | `@wonderwhy-er/desktop-commander` npm 包                     |
| **mcp-remote**                                            | **MCP Bridge / 转发器**     | 在 Tiny Agents 无法直连远程 SSE 端点时，把 stdio 请求转给远程 MCP Server    | `npx mcp-remote <url>`                                      |
| **Qwen3-8B-GGUF / Jan-Nano-4B**                           | **被调用模型**                | 作为 Lemonade Server 加载的推理后端，完成 LLM 推理                      | GGUF 格式，llamacpp 引擎                                         |
| **agent.json**                                            | **MCP Client 配置**        | 告诉 Tiny Agents 用哪个模型、连哪个 MCP Server、加载哪些 MCP Tool Server  | JSON 配置文件                                                   |
| **job\_description.md / john\_resume.md / invitation.md** | **业务数据 (上下文)**           | 由 Desktop Commander 工具在本地读写，确保敏感数据不出本机                    | 纯文本 / Markdown                                              |

至此就不更新该文档了。



Downloading Qwen3-1.7B-GGUF (unsloth/Qwen3-1.7B-GGUF:Q4_0)
Qwen3-1.7B-Q4_0.gguf: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.06G/1.06G [03:33<00:00, 4.94MB/s]
Fetching 1 files: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [03:34<00:00, 214.89s/it]
INFO:     ::1:46684 - "POST /api/v1/pull HTTP/1.1" 200 OK
INFO:     ::1:46684 - "GET /api/v1/models HTTP/1.1" 200 OK
INFO:     ::1:46684 - "GET /api/v1/models HTTP/1.1" 200 OK
Downloading user.qwen3-1-7b (Qwen/Qwen3-1.7B-GGUF:Qwen-1.7B-Q4_0.gguf)
INFO:     ::1:33510 - "POST /api/v1/pull HTTP/1.1" 500 Internal Server Err